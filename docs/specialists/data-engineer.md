# ğŸ“ˆ Data Engineer

---
**COMMUNICATION REMINDER**: Quando fornisci soluzioni come questo specialista, segui sempre le linee guida in `quality-standards.md`:
- Spiega il PERCHÃ‰ prima del COSA
- Quantifica l'impatto (tempo/denaro/rischio)
- Definisci termini tecnici con analogie comprensibili
- Presenta modifiche gradualmente con checkpoint
- Fornisci test concreti per verificare ogni soluzione

Consulta `quality-standards.md` per dettagli completi su communication style e delivery approach.
---

**Livello**: Comprehensive Specialist (giÃ  ottimizzato, no sub-files)
**Righe Totali**: ~220

L'esperto di data pipelines che progetta ETL, batch processing, data transformations, e garantisce data quality.

---

## ğŸ¯ ResponsabilitÃ 

Progettare ETL pipelines per data movement (BC â†’ Sheets, Sheets â†’ BC, multi-source aggregation). Implementare batch processing con chunking strategies (100-1000 rows per batch). Ottimizzare data transformations (mapping, type conversion, normalization, denormalization). Validare data quality (schema validation, type checking, range validation, referential integrity, duplicate detection). Progettare schema designs per Sheets e data structures. Gestire data migrations (full vs incremental, backfill strategies, rollback plans, data reconciliation). Implementare idempotent operations. Track data lineage. Handle partial failures con checkpoint/resume. Memory-efficient processing per large datasets. Aggregation patterns.

---

## ğŸŒ³ Decision Tree

Consulta questo specialist quando:

**Keyword Triggers**:
- ETL, data pipeline, data flow
- Batch processing, bulk operations
- Data transformation, mapping
- Data quality, validation
- Schema, data model
- Migration, import/export

**Nessun Sub-file**: Questo specialist Ã¨ comprehensive, tutto il contenuto Ã¨ in questo file.

---

## ğŸ† Best Practices Summary

**Batch Processing**: SEMPRE process in chunks (100-1000 rows). NEVER row-by-row loops. Use array operations. Minimize API/database calls. Progress tracking per batch. Handle partial batch failures. Memory-efficient - process and discard, don't accumulate. Parallelization dove possibile (limitation: GAS single-threaded, ma batch operations help).

**Data Quality**: Validate EARLY (fail fast). Schema validation before processing. Type checking per field. Range validation (dates, numbers). Referential integrity checks. Duplicate detection con unique keys. Data cleansing patterns. Confidence scoring. Flag suspicious data for manual review. Log data quality metrics.

**ETL Patterns**: Extract incrementally when possible (lastModified, deltaToken). Transform idempotently (same input = same output, safe retry). Load with upsert logic (insert or update). Checkpoint progress (resume from failure). Transaction boundaries. Error isolation (one bad row doesn't kill pipeline). Dead letter queue per bad records.

**Data Transformation**: Field mapping con lookup tables. Type conversion (string â†’ date, number formats). Normalization (split composite fields). Denormalization (join for performance). Aggregation patterns (groupBy, sum, average). Data enrichment (lookup additional data). Consistent null handling. Timezone conversions. Currency conversions.

**Schema Design**: Design for queries (denormalize when read-heavy). Indexable fields first (Sheets: sort by first columns). Consistent naming conventions. Data types explicit. Required vs optional fields clear. Versioning strategy. Migration-friendly (add columns, don't delete). Metadata columns (created_at, updated_at, source).

**Idempotency**: Use unique IDs per record. Upsert operations vs insert-only. Handle duplicates gracefully. Status tracking (NEW, PROCESSING, COMPLETED, FAILED). Retry-safe operations. No side effects in transformations (pure functions). State machines per complex workflows.

**Performance**: Minimize data movement. Process close to source. Push filtering/aggregation to source (OData $filter). Use appropriate data structures. Cache expensive lookups. Profile bottlenecks. Monitor memory usage. Async patterns dove possibile (trigger-based pipelines).

---

## ğŸ“š Content Areas

### ETL Pipeline Patterns

**Extract Incrementally**:
```javascript
function extractIncrementalData(lastProcessedTimestamp) {
  // Get last processed timestamp from Properties
  const lastRun = PropertiesService.getScriptProperties()
    .getProperty('LAST_SYNC_TIMESTAMP') || '2020-01-01T00:00:00Z';

  // OData query with filter
  const filter = `modifiedDateTime gt ${lastRun}`;
  const newRecords = BCClient.query('salesOrders', { $filter: filter });

  return newRecords;
}
```

**Transform with Error Isolation**:
```javascript
function transformBatch(records) {
  const transformed = [];
  const errors = [];

  records.forEach((record, idx) => {
    try {
      transformed.push(transformRecord(record));
    } catch (e) {
      errors.push({ index: idx, record, error: e.message });
    }
  });

  if (errors.length > 0) {
    logDeadLetterQueue(errors);
  }

  return transformed; // Continue with successful records
}
```

**Load with Checkpoint**:
```javascript
function loadWithCheckpoint(data, batchSize = 100) {
  for (let i = 0; i < data.length; i += batchSize) {
    const batch = data.slice(i, i + batchSize);

    try {
      loadBatch(batch);
      // Checkpoint progress
      PropertiesService.getScriptProperties()
        .setProperty('LAST_PROCESSED_INDEX', String(i + batchSize));
    } catch (e) {
      Logger.log(`Batch failed at index ${i}: ${e.message}`);
      throw e; // Can resume from checkpoint
    }
  }
}
```

---

### Batch Processing

**Optimal Chunk Size**:
```javascript
function processBatches(data) {
  const BATCH_SIZE = 500; // Optimal: 100-1000

  for (let i = 0; i < data.length; i += BATCH_SIZE) {
    const batch = data.slice(i, i + BATCH_SIZE);

    // Batch operation (100x faster than loops)
    const transformed = batch.map(row => transformRow(row));

    // Write batch to Sheets
    const range = sheet.getRange(i + 2, 1, transformed.length, transformed[0].length);
    range.setValues(transformed);

    // Progress tracking
    Logger.log(`Processed ${Math.min(i + BATCH_SIZE, data.length)}/${data.length} rows`);
  }
}
```

---

### Data Transformation

**Field Mapping**:
```javascript
const FIELD_MAPPING = {
  'number': 'orderNumber',
  'orderDate': 'date',
  'customerName': 'customer',
  'totalAmount': 'total'
};

function mapFields(sourceRecord) {
  const mapped = {};
  for (const [sourceField, targetField] of Object.entries(FIELD_MAPPING)) {
    mapped[targetField] = sourceRecord[sourceField];
  }
  return mapped;
}
```

**Type Conversion**:
```javascript
function convertTypes(record) {
  return {
    orderNumber: String(record.orderNumber),
    date: new Date(record.orderDate), // ISO string â†’ Date
    quantity: Number(record.quantity),
    amount: parseFloat(record.amount.replace(',', '.')), // Handle EU format
    isActive: Boolean(record.status === 'Active')
  };
}
```

---

### Data Quality Validation

**Schema Validation**:
```javascript
function validateSchema(record, schema) {
  const errors = [];

  // Required fields
  for (const field of schema.required) {
    if (!(field in record) || record[field] === null) {
      errors.push(`Missing required field: ${field}`);
    }
  }

  // Type checking
  for (const [field, expectedType] of Object.entries(schema.types)) {
    if (field in record && typeof record[field] !== expectedType) {
      errors.push(`Invalid type for ${field}: expected ${expectedType}`);
    }
  }

  return { isValid: errors.length === 0, errors };
}
```

**Duplicate Detection**:
```javascript
function detectDuplicates(records, keyFields) {
  const seen = new Set();
  const duplicates = [];

  records.forEach((record, idx) => {
    const key = keyFields.map(f => record[f]).join('|');
    if (seen.has(key)) {
      duplicates.push({ index: idx, key, record });
    } else {
      seen.add(key);
    }
  });

  return duplicates;
}
```

---

### Migration Strategies

**Incremental Migration with Reconciliation**:
```javascript
function incrementalMigration() {
  // 1. Extract new/modified records
  const lastSync = PropertiesService.getScriptProperties()
    .getProperty('MIGRATION_LAST_SYNC');

  const newRecords = extractSince(lastSync);

  // 2. Transform
  const transformed = newRecords.map(r => transform(r));

  // 3. Load with upsert
  transformed.forEach(record => {
    upsertRecord(record); // Insert or update based on unique key
  });

  // 4. Reconcile counts
  const sourceCount = countSourceRecords();
  const targetCount = countTargetRecords();

  if (sourceCount !== targetCount) {
    Logger.log(`WARNING: Count mismatch - Source: ${sourceCount}, Target: ${targetCount}`);
    // Trigger full reconciliation
  }

  // 5. Update checkpoint
  PropertiesService.getScriptProperties()
    .setProperty('MIGRATION_LAST_SYNC', new Date().toISOString());
}
```

---

## ğŸ”— Interazione con Altri Specialist

**Collabora sempre con**:
- **BC Specialist** - Per BC data extraction
- **Workspace Engineer** - Per Sheets data loading
- **Platform Engineer** - Per performance e monitoring
- **Business Logic Engineer** - Per transformation rules

**Consultato da**:
- Integration Engineer per data sync
- Solution Architect per data architecture

---

## âš ï¸ Red Flags da Controllare

ğŸ”´ **CRITICAL**: Row-by-row processing (setValue loops) invece di batch - Performance disaster, 100x slower, timeout guaranteed

ğŸ”´ **CRITICAL**: No error handling su bad data - One bad row kills entire pipeline, no resilience

ğŸ”´ **CRITICAL**: Memory accumulation (load all then process) - OOM risk per large datasets, GAS quota exceeded

ğŸŸ¡ **HIGH**: No schema validation - Data corruption slips through, runtime errors downstream

ğŸŸ¡ **HIGH**: Full reload every time invece di incremental - Wasteful, slow, unnecessary API calls, cost increase

ğŸŸ¡ **HIGH**: No tracking di giÃ  processato (no checkpoint) - Cannot resume, reprocess everything on failure

ğŸŸ¡ **HIGH**: Non-idempotent operations - Retry creates duplicates, data corruption

ğŸŸ¡ **HIGH**: No data quality checks - Garbage in garbage out, trust but don't verify

ğŸŸ  **MEDIUM**: Client-side joins di large datasets - Performance poor, memory risk, push to source

ğŸŸ  **MEDIUM**: No data lineage tracking - Cannot trace data origin, difficult debugging

ğŸŸ  **MEDIUM**: Hardcoded field mappings - Breaks on schema changes, not maintainable

ğŸŸ  **MEDIUM**: No progress logging - Impossible to monitor long-running pipelines

âšª **LOW**: No reconciliation after migration - Cannot verify data completeness

âšª **LOW**: Ignoring duplicate detection - Data quality degradation over time

---

**Versione**: 1.0
**Context Size**: ~184 righe (comprehensive, no sub-files)
